{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Depedencies\n",
                "import pandas as pd \n",
                "from requests import Request, Session\n",
                "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
                "import json\n",
                "import config \n",
                "api_key = config.coin_api_key\n",
                "#get the current datetime to add to the file\n",
                "from datetime import datetime\n",
                "now = datetime.now()\n",
                "injest_date = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
                "filedate = now.strftime(\"%m/%d/%Y\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define and make API Call to CMC API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "#API call to CoinMarketCap\n",
                "def api_mapping_call(number_of_entries, api_key ):\n",
                "  \"\"\"Makes api call to CMC API and pulls basic coin information for number of specified entries.\n",
                "  Returns a dictionary.\"\"\"\n",
                "  url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/map'\n",
                "  parameters = {\n",
                "      'listing_status':'active',\n",
                "      'limit': number_of_entries,\n",
                "      'sort': 'cmc_rank'\n",
                "  }\n",
                "  headers = {\n",
                "    'Accepts': 'application/json',\n",
                "    'X-CMC_PRO_API_KEY': api_key\n",
                "  }\n",
                "\n",
                "  session = Session()\n",
                "  session.headers.update(headers)\n",
                "  response = session.get(url, params=parameters)\n",
                "  data = json.loads(response.text)\n",
                "  return data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#make API call, store as dictionary\n",
                "data = api_mapping_call(100, api_key)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transform and Clean the API Call"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#grabs section of dictionary \n",
                "cmc_data = data['data']\n",
                "#puts the dict into a DF, now need to expand nested platform column\n",
                "cmc_df = pd.DataFrame.from_dict(cmc_data, orient='columns')\n",
                "#expand nest columns\n",
                "expanded_df = cmc_df['platform'].apply(pd.Series)\n",
                "#extract columns names\n",
                "cols = expanded_df.columns\n",
                "#modify column names to new names\n",
                "cols_new = [x + \"_expanded\" for x in cols]\n",
                "#create a dict mapping with old to new names\n",
                "mapping = {key1: key2 for key1, key2 in zip(cols, cols_new)}\n",
                "#implement and rename columns \n",
                "expanded_df = expanded_df.rename(columns=mapping)\n",
                "#expanded Dataframe, when expanding duplicate columns come into play ie 2 ID columns\n",
                "top_coins_df = pd.concat([cmc_df.drop(['platform'], axis= 1),expanded_df], axis= 1 )\n",
                "#add column file name\n",
                "top_coins_df[\"File_Name\"] = \"Coin_Ranking_Dim\"\n",
                "#Add column for injested at \n",
                "top_coins_df[\"injest_datetime\"] = injest_date\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Convert Dataframe to specified file type to write to storage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#convert the cleaned data into a file type to write to storage\n",
                "def convert_dataframe_to_file(dataframe, file_type: str):\n",
                "    \"\"\"Convert a pandas dataframe to csv,parquet, or text file\n",
                "    Returns transformed DF, and original DF\"\"\"\n",
                "    df_to_tranform = dataframe\n",
                "    if file_type.lower() == \"csv\":\n",
                "        df_to_tranform = df_to_tranform.to_csv(index = False )\n",
                "    if file_type.lower() == \"parquet\":\n",
                "        df_to_tranform = df_to_tranform.to_parquet()\n",
                "    if file_type.lower() == \"text\":\n",
                "        df_to_tranform = df_to_tranform.to_string(header=False, index=False)\n",
                "    else:\n",
                "        \"Specify file type: parquet, csv, or text\"\n",
                "\n",
                "    return df_to_tranform, dataframe\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "top_coins_csv, top_coin_df = convert_dataframe_to_file(top_coins_df, \"csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Write the file to specified ADLS "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from azure.storage.filedatalake import DataLakeServiceClient\n",
                "# install the following package \n",
                "# pip install azure-storage-file-datalake \n",
                "# Get the below details from your storage account\n",
                "storage_account_name = config.storage_account_name\n",
                "storage_account_key = config.storage_account_key\n",
                "container_name = \"coinlist\"\n",
                "directory_name = \"raw\"\n",
                "\n",
                "def write_to_storage(storage_account_name,storage_account_key, container_name, directory_name,dataset, pandas_dataframe, file_name):\n",
                "    \"\"\"Function to write dataframe to storage. Specicify storage account name, storage account key, container name, directory name and file name.\n",
                "    The fuction will check if container already exists, if it doesn't it will create a new container. If the already exisits, it will write the file to the specified container.\n",
                "    Dataset must be saved as specified file type(csv,txt,parquet)\n",
                "    DataFrame must be a Pandas DF, will be returned from function\n",
                "    File name must end in specificed file type (csv, txt, parquet).\"\"\"\n",
                "\n",
                "    #convert dataset input to pandas DF\n",
                "    #df = pd.DataFrame([x.split(',') for x in dataset.split('\\r\\n')])\n",
                "    #promote first row to headers\n",
                "    #df= df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
                "\n",
                "    #timestamp when function runs\n",
                "    injest_date = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
                "    #removed spaces from the datetime, backslashed dictate new folder structure withn ADLS\n",
                "    trimmed_injest_Date = injest_date.replace( \" \", \"_\").replace(' ', '_').replace(\"/\",\"_\")\n",
                "    #split the given file name into file title and file type\n",
                "    file_name_title, file_type = file_name.split('.')\n",
                "    #created storage file as it will appear in ADLS\n",
                "    storage_file = (\"{}_{}.{}\").format(file_name_title, trimmed_injest_Date, file_type)\n",
                "    \n",
                "\n",
                "    service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\n",
                "            \"https\", storage_account_name), credential=storage_account_key)\n",
                "    try:\n",
                "        file_system_client = service_client.create_file_system(file_system=container_name)\n",
                "        dir_client = file_system_client.get_directory_client(directory_name)\n",
                "        dir_client.create_directory()\n",
                "        #set data to appropriate dataframe\n",
                "        file = dataset\n",
                "        file_client = dir_client.create_file(storage_file)\n",
                "        file_client.append_data(file, 0, len(file))\n",
                "        file_client.flush_data(len(file))\n",
                "    except  :\n",
                "        #ResourceAlreadyExists\n",
                "        file_system_client = service_client.get_file_system_client(file_system=container_name)\n",
                "        dir_client = file_system_client.get_directory_client(directory_name)\n",
                "        dir_client.create_directory()\n",
                "        #set data to appropriate dataframe\n",
                "        file = dataset\n",
                "        file_client = dir_client.create_file(storage_file)\n",
                "        file_client.append_data(file, 0, len(file))\n",
                "        file_client.flush_data(len(file))   \n",
                "\n",
                "\n",
                "    return  pandas_dataframe\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataframe = write_to_storage(storage_account_name=storage_account_name, storage_account_key= storage_account_key, container_name= container_name, directory_name= directory_name,\\\n",
                "    dataset = top_coins_csv,pandas_dataframe=top_coin_df  ,file_name= \"top100coins.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Refine dataset and write to ADLS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#filters out null names\n",
                "#needed to ensure that dataset was in CSV format\n",
                "#filters out null names\n",
                "dataframe = dataframe[dataframe.name.notnull()]\n",
                "#make column selections\n",
                "desired_columns = [\"id\", \"name\", \"symbol\", \"rank\",\"File_Name\", \"injest_datetime\"]\n",
                "#filter dataset for desired columns\n",
                "refined_dataframe = dataframe[desired_columns]\n",
                "#convert dataframe to CSV to write to storage\n",
                "refined_dataframe_csv,refined_dataframe = convert_dataframe_to_file(refined_dataframe, \"csv\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Call function to write to storage and save output dataframe \n",
                "refined_coin_list = write_to_storage(storage_account_name=storage_account_name, storage_account_key= storage_account_key,\\\n",
                "container_name= \"coinlist\", directory_name= \"refined\",\\\n",
                "dataset = refined_dataframe_csv, pandas_dataframe =refined_dataframe, file_name= \"top100coins.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "89bbb57337a288069efe3ede2e44e349d48d03d33172adbe5738fcfdbda01bd0"
        },
        "kernelspec": {
            "display_name": "Python 3.9.7 ('base')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
